{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f2d569-b73d-4403-9ad9-e0e8a46f4706",
   "metadata": {},
   "source": [
    "# RAG-based AI App About Ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf155955-3575-4225-9afb-c929b59b2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e3c632-2670-4f0b-befd-8c6ab3777c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.40.1\n"
     ]
    }
   ],
   "source": [
    "print(sqlite3.sqlite_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591a866-b5d3-4ede-ac9c-c782d1e74c96",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908189e7-4c2f-4386-ac1a-b2749cfa40e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb448214-5ddc-4ff5-bd92-1745ca2b7a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "import shutil\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54862cbc-da82-4133-9570-b988c14cb49b",
   "metadata": {},
   "source": [
    "### Configure Gemini LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4728a9-65be-4545-8ff3-f7a2997c8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt=\"do you know ana?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a91a567-7049-4f06-a712-4ca8aa8c44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genai.configure(api_key=os.environ['GEMINI_API_KEY'])\n",
    "# model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# response = model.generate_content(\n",
    "#     prompt,\n",
    "#     generation_config=genai.types.GenerationConfig(\n",
    "#         # max_output_tokens=140,\n",
    "#         # temperature=0.7,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
    "                         google_api_key=os.environ['GEMINI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ba45e1-aaae-4e72-ae84-9da3782a0100",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d18cf-6e3d-4a36-94bd-c74082b46329",
   "metadata": {},
   "source": [
    "### Create vector DB from personal PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f532317b-8b58-4316-8e53-d7cb5444f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data_sources\"\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "gpt4all_embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs\n",
    ")\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # clear previous db\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # create db\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, gpt4all_embeddings, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1099b564-a1a6-4f37-86bc-fd471edfdb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 3 documents into 50 chunks.\n",
      "Saved 50 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()\n",
    "doc_chunks = split_text(documents)\n",
    "save_to_chroma(doc_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10475827-7ab9-442b-8e8c-558a207b9cdd",
   "metadata": {},
   "source": [
    "### Retrieve similar vectors as the prompt from the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19c87c81-a988-4e65-94dc-91e41d0cd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH,\n",
    "            embedding_function=gpt4all_embeddings)\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# r_docs = retriever.invoke(\"what are ana's qualifications?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d95f19-8092-4994-a150-41a496f4a6fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt_embedding = gpt4all_embeddings.embed_query(\"who is ana?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ca69a3-d5b1-4c95-aa19-8bf2c3464b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data_sources\\\\Personal Profile.pdf', 'start_index': 0}, page_content='Name: Ana Vasquez\\n\\nPersonal Goal:\\n\\nHealth: Keep a healthy lifestyle so much that I am still fit by 65 y.o. - Career: Become an NLP specialist in the Philippines.\\n\\nHobbies: -'),\n",
       " Document(metadata={'source': 'data_sources\\\\PROFILE DS AI - Ana Vasquez.pdf', 'start_index': 0}, page_content='ANA VASQUEZ avasquez.msds2024@aim.edu | linkedin.com/in/ana-p-vasquez\\n\\ngithub.com/helloanavee | +63 919 001 5061'),\n",
       " Document(metadata={'source': 'data_sources\\\\LinkedIn Profile.pdf', 'start_index': 182}, page_content=\"Certifications\\n\\nData Analytics\\n\\nHonors-Awards\\n\\nSpecial Award: Awesome Attitude Award\\n\\nAna Vasquez\\n\\nData Science | NLP | Gen AI | MS Data Science at AIM Metro Manila, National Capital Region, Philippines\\n\\nSummary\\n\\nI'm a Data Scientist passionate about Natural Language Processing,\\n\\nGen AI, and LLMs.\"),\n",
       " Document(metadata={'source': 'data_sources\\\\Personal Profile.pdf', 'start_index': 659}, page_content='Current Role:\\n\\n\\n\\nI am an Data Science Intern for Netopia AI. From Netopia AI’s website:')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7100c95-7044-4bcc-bd4b-789b0451ccc2",
   "metadata": {},
   "source": [
    "### Reformulate the question to give context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9b993b1-d64a-40df-b838-a68fb3c5bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e95854-46f7-43a0-84fe-ad9eb424a382",
   "metadata": {},
   "source": [
    "### Answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b20d99a4-83c8-4bd2-ad64-6f8f7bb8399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "qa_system_prompt = \"\"\"\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "Use three to five sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ") \n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee54e93c-03a5-4cab-896b-af25eee997ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# We define a dict representing the state of the application.\n",
    "# This state has the same input and output keys as `rag_chain`.\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# We then define a simple node that runs the `rag_chain`.\n",
    "# The `return` values of the node update the graph state, so here we just\n",
    "# update the chat history with the input message and response.\n",
    "def call_model(state: State):\n",
    "    response = rag_chain.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# Our graph consists only of one node:\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Finally, we compile the graph with a checkpointer object.\n",
    "# This persists the state, in this case in memory.\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94edeb75-7be4-4cb1-b5e4-7c3aa2245fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ana is a Data Science Intern at Netopia AI, where she collaborates with clients to create data projects using SQL and Looker Studio. Her skills include Python, SQL, NLP tools, and a focus on end-to-end machine learning solutions. She has experience in various industries, including marketing, HR, real estate, health, and finance. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "result = app.invoke(\n",
    "    {\"input\": \"Can you explain more on her role in data science?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9acafaa0-9547-4934-8f41-22f3ea9192f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "And what else?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The individual has a strong background in software engineering and data science, demonstrated by their experience at Magis Solutions and their current internship at Netopia AI. They have expertise in machine learning, data analysis, and software development, with a proven track record of delivering business value through their projects.  Their skills include Python, SQL, NLP tools, and a focus on end-to-end machine learning solutions.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is ana good at?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ana is skilled in data science, machine learning, and software engineering. She has a strong background in Python, SQL, and NLP tools. Her experience includes working as a data science consultant, data analyst, and software engineer, focusing on delivering business value through end-to-end machine learning solutions.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you explain more on her role in data science?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ana is a Data Science Intern at Netopia AI, where she collaborates with clients to create data projects using SQL and Looker Studio. Her skills include Python, SQL, NLP tools, and a focus on end-to-end machine learning solutions. She has experience in various industries, including marketing, HR, real estate, health, and finance.\n"
     ]
    }
   ],
   "source": [
    "chat_history = app.get_state(config).values[\"chat_history\"]\n",
    "for message in chat_history:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8665c28e-0673-4ee1-8357-e04def5f79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "#     if session_id not in store:\n",
    "#         store[session_id] = load_session_history(session_id)\n",
    "#     return store[session_id]\n",
    "\n",
    "# # Function to load chat history\n",
    "# def load_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "#     db = next(get_db())\n",
    "#     chat_history = ChatMessageHistory()\n",
    "#     try:\n",
    "#         session = db.query(Session).filter(Session.session_id == session_id).first()\n",
    "#         if session:\n",
    "#             for message in session.messages:\n",
    "#                 chat_history.add_message({\"role\": message.role, \"content\": message.content})\n",
    "#     except SQLAlchemyError:\n",
    "#         pass\n",
    "#     finally:\n",
    "#         db.close()\n",
    "\n",
    "#     return chat_history\n",
    "\n",
    "# ### Statefully manage chat history ###\n",
    "# store = {}\n",
    "\n",
    "# conversational_rag_chain = RunnableWithMessageHistory(\n",
    "#     rag_chain,\n",
    "#     get_session_history,\n",
    "#     input_messages_key=\"input\",\n",
    "#     history_messages_key=\"chat_history\",\n",
    "#     output_messages_key=\"answer\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867de01-760e-4207-bca0-f5dd241f583a",
   "metadata": {},
   "source": [
    "### Prompt the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e71d2027-bff6-4936-ae43-93af1927eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based mostly only on the following context:\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\"\"\"\n",
    "\n",
    "query_text = \"Should I hire ana as an AI Engineer?\"\n",
    "\n",
    "context_text = \"\\n\\n---\\n\\n\".join([r_doc.page_content for r_doc in r_docs])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461994d2-a217-40e1-9695-60d66cf6b2d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based mostly only on the following context:\n",
      "\n",
      "CONTEXT: Name: Ana Vasquez\n",
      "\n",
      "Personal Goal:\n",
      "\n",
      "Health: Keep a healthy lifestyle so much that I am still fit by 65 y.o. - Career: Become an NLP specialist in the Philippines.\n",
      "\n",
      "Hobbies: -\n",
      "\n",
      "---\n",
      "\n",
      "provides a normalized measure of performance, allowing for comparison\n",
      "\n",
      "across different assignments with varying point values. Only valid score ratio\n",
      "\n",
      "between 0 and 1 is considered. This ensures that scores exceeding 1, possibly\n",
      "\n",
      "---\n",
      "\n",
      "through regular code reviews.\n",
      "\n",
      "Education\n",
      "\n",
      "Asian Institute of Management\n",
      "\n",
      "Master of Science - MS, Data Science · (July 2023 - September 2024)\n",
      "\n",
      "De La Salle University\n",
      "\n",
      "Bachelor’s Degree, Psychology\n",
      "\n",
      "Eskwelabs\n",
      "\n",
      "Certificate, Data Analytics Bootcamp · (October 2021 - December 2021)\n",
      "\n",
      "---\n",
      "\n",
      "sources, ensuring a cohesive dataset for evaluation. Data anonymization is conducted\n",
      "\n",
      "to protect privacy, followed by comprehensive preprocessing, which includes\n",
      "\n",
      "removing non-English rows, converting text to lowercase, eliminating stopwords,\n",
      "\n",
      "QUESTION: Should I hire ana as an AI Engineer?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66846f97-7ab5-4d82-a7eb-a588610b9e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context doesn't offer enough information to make a decision about hiring Ana as an AI Engineer. \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **No AI Engineering Experience:** The context highlights Ana's background in data science and psychology, but there's no mention of any experience or skills related to AI engineering.\n",
      "* **Goal is NLP:** While NLP (Natural Language Processing) is a branch of AI, Ana's goal is to become an NLP specialist, not an AI engineer in general. \n",
      "* **Context Focuses on Education:** The provided information focuses on Ana's education and a random snippet about normalized score ratios. This doesn't give a clear picture of her technical abilities or suitability for an AI engineering role.\n",
      "\n",
      "**To make a hiring decision, you'd need to:**\n",
      "\n",
      "* **Assess Ana's technical skills:** Evaluate her programming abilities, knowledge of AI algorithms, and experience with AI development tools.\n",
      "* **Determine her AI engineering experience:** Look for past projects or roles related to AI development.\n",
      "* **Consider her career goals:** Understand if her goal is to become an AI engineer, and if her career path aligns with the requirements of the role. \n",
      "\n",
      "The context provided is insufficient to determine if Ana would be a good fit for an AI engineering position. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d5cda-2341-4519-a98e-afa6656e1265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
