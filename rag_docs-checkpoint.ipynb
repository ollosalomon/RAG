{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1c72e6f-2f74-41ab-8374-5614a16b9d4d",
   "metadata": {},
   "source": [
    "# RAG-based OLLO QNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4bc31-15ec-4366-836a-affba6d21fda",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbbed1-b232-4d51-8a7b-7ef75e2f0865",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59ea7a-016f-44d0-a8aa-fef742b3c0b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891daee4-c097-42f2-b689-6a7080ed3953",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f30da-6485-4936-801e-f631e8403be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "import os\n",
    "import shutil\n",
    "from langchain.evaluation import load_evaluator\n",
    "from pydantic import BaseModel, GetJsonSchemaHandler\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import google.generativeai as genai\n",
    "from langchain.chains import ConversationChain\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b29aaf28-776c-41c8-8a83-5a3d75746129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "# from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "# from langchain.chains.conversation.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755e9f2b-238d-4f49-af28-56a611f313f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc0c2f-b6df-489a-9ba6-8edb2882e186",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### create chunks from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5268d82-e7e7-4b7f-8d8d-9a67ceb4992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data_sources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53160d13-221e-41ac-a241-7f850b41a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ddd05b7-d91a-484b-a31e-d0dea7d20462",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da0f8e0-be84-4db5-bc28-1295c2d84daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    \n",
    "    document = chunks[3]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b795f-2e6c-460a-bd03-a83b6a0cfd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ffc4e-d7c1-4250-a18d-43d478f62a0d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff7faaa-a3dd-4a79-993e-e69496ec08b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### save chunks to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a11a09-8abb-4ec2-bd52-88ed61886491",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "gpt4all_embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs\n",
    ")\n",
    "    \n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # clear previous db\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # create db\n",
    "    db = Chroma.from_documents(\n",
    "        doc_chunks, gpt4all_embeddings, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6574c20-e703-4246-890c-c51cb87d75d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 19 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "save_to_chroma(doc_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e45e86-40dc-49ab-9d47-cf31cd996654",
   "metadata": {
    "tags": []
   },
   "source": [
    "### query the closest texts from the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d51b6d-5a5e-48b5-952e-451a49f75add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the same embedding function\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "gpt4all_embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs\n",
    ")\n",
    "\n",
    "# prepare the db\n",
    "db = Chroma(persist_directory=CHROMA_PATH,\n",
    "            embedding_function=gpt4all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ac49c90-fef0-4d8d-b266-231867278388",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"should i hire her as a data scientist?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87908381-f339-4a71-a442-cc3661db384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5a3bb-71a6-46d0-af1b-014164e2956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9ce14-0986-423a-8dc2-7941b026af09",
   "metadata": {
    "tags": []
   },
   "source": [
    "### create the prompt with my custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1b51d1d9-5878-4707-8d51-4939e464a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Given the following context and a question, generate an answer based on this context only.\n",
    "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a43c88e0-f9a5-4a30-a40e-6863306ff520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT_TEMPLATE = \"\"\"\n",
    "# Answer the question based only on the following context:\n",
    "\n",
    "# {context}\n",
    "\n",
    "# ---\n",
    "\n",
    "# Answer the question based on the above context: {question}\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43fd8ba3-c1ae-40ce-88a0-cc38c5522aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba69b435-f0bd-4477-86c0-ecf890ef9332",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00e424-c7b3-4e7f-b606-f508b5dd32c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### prompt the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4f2149e-d823-4ae9-823f-86450f82db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GEMINI_KEY.txt', 'r') as file:\n",
    "    exec(file.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffac7d-ceba-4e93-ab27-935f59b6c596",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "genai.configure(api_key=API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98323117-11c9-491a-af8d-8fa6927daa10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b30d2f1-30a8-4881-ad63-23d49074d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52d82604-27ed-4959-978e-946a33b7a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9df445d9-6146-43f7-bcea-b9256ff23382",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f6744-222c-4421-b938-05a8c8054517",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e60af-cf89-4c23-b20a-3ea5703a72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryMemory(llm=llm),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cab2a8-61a2-494a-b72b-ae73f72333c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_with_summary = conversation_with_summary.invoke(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba923a06-70bd-4da7-a1ab-de4687baca9e",
   "metadata": {},
   "source": [
    "### prompt with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2e38612-e5fd-441a-81af-89db2a74c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# llm = ChatOpenAI(openai_api_key=openai_api_key)\n",
    "\n",
    "# convert the output of the chatmodel into pure text\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e97aa0cd-052d-469d-8c64-8b81b4ebde8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever takes the question then compares with numeric vectors in the db and return the similar text\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398fc7bc-e224-4f20-8ce6-bbd56f03f30b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d75cb04-6333-46a0-9cd7-53bac2c802d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "instruction_to_system = \"\"\"\n",
    "Given a chat history and the latest user question\n",
    "which might reference context in the chat history, formulate a standalone question\n",
    "which can be understood without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "question_maker_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", instruction_to_system),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_chain = question_maker_prompt | llm | StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a8cea-9199-40cc-9768-822b373fdedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
